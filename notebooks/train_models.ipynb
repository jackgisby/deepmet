{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model training\n",
    "\n",
    "Training a model for the one-class classification of metabolites. Replicates the training process of the DeepMet model, using a subset of the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from shutil import copytree\n",
    "from pyod.models import ocsvm, iforest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tests.utils import get_normal_non_normal_subsets\n",
    "from deepmet.auxiliary import get_fingerprints_from_meta, select_features, Config\n",
    "from deepmet.datasets import load_training_dataset\n",
    "from deepmet.workflows import train_single_model, train_likeness_scorer, get_likeness_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We extracted compounds from the HMDB and ZINC12 databases, subject to the following constraints:\n",
    " - Exact mass filter: 100Da < exact mass \\> 800Da\n",
    " - Heavy atom filter: heavy atoms \\>= 4\n",
    " - RDKit molecular sanitization\n",
    "\n",
    "The entire set of compounds passing these filters in HMDB were retained while a random sample of 20,000 compounds were taken from ZINC12. The smiles for these compounds are available in the `deepmet/data/test_set` folder along with their respective compound IDs.\n",
    "\n",
    "Below, we create the folder `notebook_results` and make a copy of the compound lists. These smiles must be converted to molecular fingerprints which are used as input to the models; however, parsing the smiles and converting them to fingerprints is a particularly time-consuming step. For the purposes of this notebook, we take subsets of 500 HMDB (\"normal\") structures and 50 ZINC12 (\"non-normal\") compounds. These are randomly selected using `get_normal_non_normal_subsets`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Path to write results\n",
    "results_path = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"notebook_results\")\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.mkdir(results_path)\n",
    "\n",
    "# Copy input data to the results folder\n",
    "copytree(\n",
    "    os.path.join(os.path.dirname(os.path.abspath(\"\")), \"deepmet\", \"data\"),\n",
    "    os.path.join(results_path, \"data\")\n",
    ")\n",
    "\n",
    "# Seed to be used for loading the dataset and training models\n",
    "seed = 1\n",
    "\n",
    "# Location of the \"normal\" and \"non-normal\" smiles\n",
    "normal_meta_path, non_normal_meta_path = get_normal_non_normal_subsets(results_path, seed=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The DeepMet package is based on the DeepSVDD model developed by Ruff et al., 2018:\n",
    " - GitHub: https://github.com/lukasruff/Deep-SVDD-PyTorch\n",
    " - ICML paper: http://proceedings.mlr.press/v80/ruff18a.html\n",
    "\n",
    "The authors demonstrate the model for the detection of anomalous images. DeepMet uses the DeepSVDD approach to identify anomalous compounds - specifically, we trained the model on metabolites to generate \"metabolite-likeness\" scores. DeepMet does, however, allow users to re-train the model for any class of compounds; therefore, likeness scores can be generated for any type of structures.\n",
    "\n",
    "There are two key workflows in the DeepMet package:\n",
    " - `train_likeness_scorer`: implements the workflow for training a DeepMet model\n",
    " - `get_likeness_scores`: uses a pre-trained DeepMet model to generate metabolite-likeness scores for new compounds\n",
    "\n",
    "For the purposes of this vignette, we will first carry out the individual steps of the `train_likeness_scorer` workflow manually. Then, we will generate an identical model using `train_likeness_scorer` and compare it to two other one-class classification algorithms. Finally, we will take the stored model weights and those of the full model generated in the DeepMet paper and re-score the subset of compounds using `get_likeness_scores`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "normal_fingerprints_path = os.path.join(results_path, \"normal_fingerprints.csv\")\n",
    "non_normal_fingerprints_path = os.path.join(results_path, \"non_normal_fingerprints.csv\")\n",
    "\n",
    "# Takes the smiles and converts them to molecular fingerprints for each compound class\n",
    "processed_normal_fingerprints_path = get_fingerprints_from_meta(normal_meta_path, normal_fingerprints_path)\n",
    "processed_non_normal_fingerprints_path = get_fingerprints_from_meta(non_normal_meta_path, non_normal_fingerprints_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we set the training options for the DeepMet model. The learning rate was selected in the DeepMet paper as it was associated with the minimum loss on the validation dataset. Note that these parameters were set based on the full training dataset, so may not lead to a model with comparable performance based on the subset we are using here.\n",
    "\n",
    "We use a set transformer architecture developed by Lee et al., 2019 (http://proceedings.mlr.press/v97/lee19d.html), which was also used by Vriza et al., 2020 (https://pubs.rsc.org/en/content/articlelanding/2021/sc/d0sc04263c) to predict co-crystal pairs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Settings required by the DeepMet model\n",
    "cfg = Config({\n",
    "    \"net_name\": \"cocrystal_transformer\",\n",
    "    \"objective\": \"one-class\",\n",
    "    \"nu\": 0.1,\n",
    "    \"rep_dim\": 200,\n",
    "    \"seed\": seed,\n",
    "    \"optimizer_name\": \"amsgrad\",\n",
    "    \"lr\": 0.000155986,\n",
    "    \"n_epochs\": 20,\n",
    "    \"lr_milestones\": tuple(),\n",
    "    \"batch_size\": 25,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"pretrain\": False,\n",
    "    \"in_features\": 2746,\n",
    "    \"device\": \"cpu\"\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While we have generated the raw molecular fingerprints, these include many poorly balanced and redundant features. We therefore use `select_features` to these prior to model training.\n",
    "\n",
    "The data is then loaded into a torch-compatible format using `load_training_dataset`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "normal_fingerprints_path, non_normal_fingerprints_paths, selected_features = select_features(\n",
    "        normal_fingerprints_path=normal_fingerprints_path,\n",
    "        normal_fingerprints_out_path=os.path.join(results_path, \"selected_normal_fingerprints.csv\"),\n",
    "        non_normal_fingerprints_paths=non_normal_fingerprints_path,\n",
    "        non_normal_fingerprints_out_paths=os.path.join(results_path, \"selected_non_normal_fingerprints.csv\")\n",
    ")\n",
    "\n",
    "cfg.settings[\"selected_features\"] = selected_features\n",
    "\n",
    "# select_features allows for the simultaneous selection of multiple non-normal datasets\n",
    "# we only have a single non-normal ZINC12 set here, which we will use to evaluate the final model\n",
    "non_normal_fingerprints_path = non_normal_fingerprints_paths[0]\n",
    "\n",
    "dataset, dataset_labels, validation_dataset = load_training_dataset(\n",
    "    normal_dataset_path=normal_fingerprints_path,\n",
    "    normal_meta_path=normal_meta_path,\n",
    "    non_normal_dataset_path=non_normal_fingerprints_path,\n",
    "    non_normal_dataset_meta_path=non_normal_meta_path,\n",
    "    seed=seed,\n",
    "    validation_split=0.8,\n",
    "    test_split=0.9\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the dataset loaded, we can now train the model.  We can use AUC to evaluate the model's discriminative capacity for metabolites vs ZINC12 compounds - but, importantly, AUC was not used for hyperparameter optimisation as the validation set does not contain any \"non-normal\" compounds.\n",
    "\n",
    "The AUC is relatively poor compared to that reported in the DeepMet paper as we are using a subset of the training data and we did not re-optimise model hyperparameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackg\\Miniconda3\\envs\\DeepMet\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "AUC on test set: 0.9404\n"
     ]
    }
   ],
   "source": [
    "# Train the model (loss is calculated on the 'normal' validation set for parameter tuning)\n",
    "deep_met_model = train_single_model(cfg, validation_dataset)\n",
    "\n",
    "# Test using separate test dataset (includes the ZINC12 set of 'non-normal' compounds)\n",
    "deep_met_model.test(dataset, device=cfg.settings[\"device\"])\n",
    "\n",
    "initial_auc = round(deep_met_model.results[\"test_auc\"], 4)\n",
    "print(\"AUC on test set: \" + str(initial_auc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of going through each of these steps, the `train_likeness_scorer` function can train a model from scratch from the original smiles. Here, we re-train the model using the same settings as above and the pre-calculated fingerprints."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Log file is C:\\Users\\jackg\\OneDrive\\Documents\\Work\\Imperial_PhD\\Side_projects\\DeepMet\\scripting\\DeepMet\\notebook_results/log.txt.\n",
      "INFO:root:Export path is C:\\Users\\jackg\\OneDrive\\Documents\\Work\\Imperial_PhD\\Side_projects\\DeepMet\\scripting\\DeepMet\\notebook_results.\n",
      "INFO:root:Network: cocrystal_transformer\n",
      "INFO:root:The filtered normal fingerprint matrix path is C:\\Users\\jackg\\OneDrive\\Documents\\Work\\Imperial_PhD\\Side_projects\\DeepMet\\scripting\\DeepMet\\notebook_results\\normal_fingerprints_processed.csv.\n",
      "INFO:root:The filtered normal meta is C:\\Users\\jackg\\OneDrive\\Documents\\Work\\Imperial_PhD\\Side_projects\\DeepMet\\scripting\\DeepMet\\notebook_results\\normal_meta.csv.\n",
      "INFO:root:The filtered non-normal fingerprint matrix path is C:\\Users\\jackg\\OneDrive\\Documents\\Work\\Imperial_PhD\\Side_projects\\DeepMet\\scripting\\DeepMet\\notebook_results\\non_normal_fingerprints_processed.csv.\n",
      "INFO:root:The filtered non-normal meta is C:\\Users\\jackg\\OneDrive\\Documents\\Work\\Imperial_PhD\\Side_projects\\DeepMet\\scripting\\DeepMet\\notebook_results\\non_normal_meta.csv.\n",
      "INFO:root:Deep SVDD objective: one-class\n",
      "INFO:root:Nu-parameter: 0.10\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of input features: 2746\n",
      "INFO:root:Set seed to 1.\n",
      "INFO:root:Training optimizer: amsgrad\n",
      "INFO:root:Training learning rate: 0.000155986\n",
      "INFO:root:Training epochs: 20\n",
      "INFO:root:Training batch size: 2000\n",
      "INFO:root:Training weight decay: 1e-05\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "C:\\Users\\jackg\\Miniconda3\\envs\\DeepMet\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:  Epoch 1/20\t Time: 0.245\t Loss: 6.08613014\n",
      "INFO:root:  Epoch 2/20\t Time: 0.276\t Loss: 41.17364883\n",
      "INFO:root:  Epoch 3/20\t Time: 0.210\t Loss: 19.69269943\n",
      "INFO:root:  Epoch 4/20\t Time: 0.223\t Loss: 15.97539139\n",
      "INFO:root:  Epoch 5/20\t Time: 0.318\t Loss: 10.52810764\n",
      "INFO:root:  Epoch 6/20\t Time: 0.256\t Loss: 8.86841393\n",
      "INFO:root:  Epoch 7/20\t Time: 0.214\t Loss: 6.90053034\n",
      "INFO:root:  Epoch 8/20\t Time: 0.222\t Loss: 6.01496744\n",
      "INFO:root:  Epoch 9/20\t Time: 0.203\t Loss: 5.74058676\n",
      "INFO:root:  Epoch 10/20\t Time: 0.201\t Loss: 4.79901552\n",
      "INFO:root:  Epoch 11/20\t Time: 0.201\t Loss: 4.31278038\n",
      "INFO:root:  Epoch 12/20\t Time: 0.217\t Loss: 4.21338844\n",
      "INFO:root:  Epoch 13/20\t Time: 0.220\t Loss: 3.69345236\n",
      "INFO:root:  Epoch 14/20\t Time: 0.202\t Loss: 3.26012659\n",
      "INFO:root:  Epoch 15/20\t Time: 0.201\t Loss: 2.85929441\n",
      "INFO:root:  Epoch 16/20\t Time: 0.205\t Loss: 2.58716440\n",
      "INFO:root:  Epoch 17/20\t Time: 0.241\t Loss: 2.53407598\n",
      "INFO:root:  Epoch 18/20\t Time: 0.224\t Loss: 2.38453221\n",
      "INFO:root:  Epoch 19/20\t Time: 0.207\t Loss: 2.20460486\n",
      "INFO:root:  Epoch 20/20\t Time: 0.197\t Loss: 2.00281143\n",
      "INFO:root:Training time: 4.502\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test set Loss: 2.26239038\n",
      "INFO:root:Testing time: 0.010\n",
      "INFO:root:Finished testing.\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test set Loss: 3.42835784\n",
      "INFO:root:Testing time: 0.014\n",
      "INFO:root:Test set AUC: 94.04%\n",
      "INFO:root:Finished testing.\n",
      "INFO:root:The AUC on the test dataset is: 0.9403999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "AUC on test set: 0.9404\n"
     ]
    }
   ],
   "source": [
    "deep_met_model = train_likeness_scorer(\n",
    "    normal_meta_path=normal_meta_path,\n",
    "    non_normal_meta_path=non_normal_meta_path,\n",
    "    normal_fingerprints_path=normal_fingerprints_path,\n",
    "    non_normal_fingerprints_path=non_normal_fingerprints_path,\n",
    "    results_path=results_path,\n",
    "    net_name=cfg.settings[\"net_name\"],\n",
    "    objective=cfg.settings[\"objective\"],\n",
    "    nu=cfg.settings[\"nu\"],\n",
    "    rep_dim=cfg.settings[\"rep_dim\"],\n",
    "    device=cfg.settings[\"device\"],\n",
    "    seed=seed,\n",
    "    optimizer_name=cfg.settings[\"optimizer_name\"],\n",
    "    lr=cfg.settings[\"lr\"],\n",
    "    n_epochs=cfg.settings[\"n_epochs\"],\n",
    "    lr_milestones=cfg.settings[\"lr_milestones\"],\n",
    "    batch_size=cfg.settings[\"batch_size\"],\n",
    "    weight_decay=cfg.settings[\"weight_decay\"],\n",
    "    validation_split=0.8,\n",
    "    test_split=0.9\n",
    ")\n",
    "\n",
    "workflow_auc = round(deep_met_model.results[\"test_auc\"], 4)\n",
    "assert initial_auc == workflow_auc\n",
    "\n",
    "print(\"AUC on test set: \" + str(workflow_auc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackg\\Miniconda3\\envs\\DeepMet\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:263: UserWarning: max_samples (1000) is greater than the total number of samples (400). max_samples will be set to n_samples for estimation.\n",
      "  warn(\"max_samples (%s) is greater than the \"\n"
     ]
    }
   ],
   "source": [
    "iforest_model = iforest.IForest(\n",
    "    contamination=0.1,\n",
    "    n_estimators=400,\n",
    "    behaviour=\"new\",\n",
    "    random_state=seed,\n",
    "    max_samples=1000\n",
    ")\n",
    "\n",
    "ocsvm_model = ocsvm.OCSVM(\n",
    "    contamination=0.1,\n",
    "    kernel=\"rbf\",\n",
    "    nu=0.1,\n",
    "    gamma=0.00386\n",
    ")\n",
    "\n",
    "x_train = validation_dataset.train_set.dataset.data[validation_dataset.train_set.indices]\n",
    "\n",
    "iforest_model.fit(x_train)\n",
    "\n",
    "ocsvm_model.fit(x_train)\n",
    "\n",
    "pickle.dump(iforest_model, open(os.path.join(results_path, \"iforest_model.pkl\"), \"wb\"))\n",
    "pickle.dump(ocsvm_model, open(os.path.join(results_path, \"ocsvm_model.pkl\"), \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9492\n",
      "0.9531999999999999\n",
      "Isolation forest AUC: 0.95\n",
      "OC-SVM AUC: 0.95\n"
     ]
    }
   ],
   "source": [
    "x_test = dataset.test_set.dataset.data[dataset.test_set.indices]\n",
    "labels_test = dataset.test_set.dataset.labels[dataset.test_set.indices]\n",
    "\n",
    "print(\"Isolation forest AUC: \" + str(round(roc_auc_score(labels_test, iforest_model.decision_function(x_test)), 4)))\n",
    "print(\"OC-SVM AUC: \" + str(round(roc_auc_score(labels_test, ocsvm_model.decision_function(x_test)), 4)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having trained a DeepMet model, we may want to re-use it in the future to score new compounds. Alternatively, we can use the model that was trained in the DeepMet paper (based on the full set of endogenous metabolites in HMDB) which is likely to generalise better to new compounds.\n",
    "\n",
    "In the code above, we split the data into training, validation and test sets. However here, we are re-using the entire subset of compounds to demonstrate the `get_likeness_scores` function - so these AUC scores are likely to be optimistic."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"notebook\": {},  # scores for the model generated in this notebook using a limited dataset\n",
    "    \"paper\": {}      # scores for the model generated in the DeepMet paper on the full dataset\n",
    "}\n",
    "\n",
    "for model_name in (\"notebook\", \"paper\"):\n",
    "    for dataset_name, meta_path in ((\"normal\", \"non-normal\"), normal_meta_path, non_normal_meta_path):\n",
    "\n",
    "        if model_name == \"paper\":\n",
    "            load_model, load_config = None, None\n",
    "        else:\n",
    "            load_model, load_config = os.path.join(results_path, \"model.tar\"), os.path.join(results_path, \"config.json\")\n",
    "\n",
    "        scores[model_name][dataset_name] = get_likeness_scores(\n",
    "            non_normal_meta_path,\n",
    "            results_path,\n",
    "            load_model=load_model,\n",
    "            load_config=load_config,\n",
    "            device=cfg.settings[\"device\"]\n",
    "        )\n",
    "\n",
    "    all_model_scores = scores[model_name][\"normal\"] + scores[model_name][\"non-normal\"]\n",
    "    all_model_labels = [0] * len(scores[model_name][\"normal\"]) + [0] * len(scores[model_name][\"non-normal\"])\n",
    "\n",
    "    print(\"AUC for \" + model_name + \"model: \" + str(roc_auc_score(all_model_labels, all_model_scores)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"notebook\": {},  # scores for the model generated in this notebook using a limited dataset\n",
    "    \"paper\": {}      # scores for the model generated in the DeepMet paper on the full dataset\n",
    "}\n",
    "\n",
    "for model_name in (\"notebook\", \"paper\"):\n",
    "    for dataset_name, meta_path in ((\"normal\", \"non-normal\"), normal_meta_path, non_normal_meta_path):\n",
    "\n",
    "        if model_name == \"paper\":\n",
    "            load_model, load_config = None, None\n",
    "        else:\n",
    "            load_model, load_config = os.path.join(results_path, \"model.tar\"), os.path.join(results_path, \"config.json\")\n",
    "\n",
    "        scores[model_name][dataset_name] = get_likeness_scores(\n",
    "            non_normal_meta_path,\n",
    "            results_path,\n",
    "            load_model=load_model,\n",
    "            load_config=load_config,\n",
    "            device=cfg.settings[\"device\"]\n",
    "        )\n",
    "\n",
    "    all_model_scores = scores[model_name][\"normal\"] + scores[model_name][\"non-normal\"]\n",
    "    all_model_labels = [0] * len(scores[model_name][\"normal\"]) + [0] * len(scores[model_name][\"non-normal\"])\n",
    "\n",
    "    print(\"AUC for \" + model_name + \"model: \" + str(roc_auc_score(all_model_labels, all_model_scores)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-5d6a92ec",
   "language": "python",
   "display_name": "PyCharm (DeepMet)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}